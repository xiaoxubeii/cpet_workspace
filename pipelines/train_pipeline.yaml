metadata:
  name: "cpet_util_pipeline"
  vars:
    RUNS_DIR: "{EXPERIMENT_HOME}/sota"
    RUN_DIR: "{RUNS_DIR}/{RUN_TS}"
    BEST_DIR: "{RUNS_DIR}/best"
    ARTIFACTS_DIR: "{EXPERIMENT_HOME}/artifacts"
    DATASET_DIR: "{ARTIFACTS_DIR}/cpet_dataset/"
    model_list: "{model_list}"
    data_file: "{DATASET_DIR}/cpet_dataset.h5"
    subset_data_file: "{DATASET_DIR}/cpet_dataset_medium.h5"
    learning_rate: "5e-5"
    weight_decay: "1e-5"
    grad_clip: "0.3"
    num_epochs: "30"
    batch_size: "64"
    model_overrides: ""
  envs:
    PYTHONUNBUFFERED: "1"
    PYTHONPATH: "{EXPERIMENT_HOME}/src"
    CPETX_PROJECT_ROOT: "{EXPERIMENT_HOME}"
    CONFIGS_DIR: "{EXPERIMENT_HOME}/configs"

steps:
  - id: prepare-run
    type: shell
    description: "Create utility pipeline directories."
    vars:
      STEP_DIR: "{RUN_DIR}/prepare-run"
    command: |
      set -e
      mkdir -p "{RUN_DIR}" "{RUN_DIR}/reports" "{RUN_DIR}/train" "{STEP_DIR}"
      echo "{model_list}" > "{STEP_DIR}/models.txt"
      python - <<'PY'
      import json
      from pathlib import Path

      overrides_raw = """{model_overrides}""".strip()
      if overrides_raw:
          try:
              overrides = json.loads(overrides_raw)
          except json.JSONDecodeError as exc:
              raise SystemExit(f"[prepare-run] Invalid model_overrides JSON: {exc}")
      else:
          overrides = {}
      Path("{STEP_DIR}/model_overrides.json").write_text(
          json.dumps(overrides, indent=2, ensure_ascii=False), encoding="utf-8"
      )
      PY

  - id: train-models
    type: shell
    depends_on: [prepare-run]
    description: "Train one or many models with shared hyperparameters."
    vars:
      STEP_DIR: "{RUN_DIR}/train"
    command: |
      set -e
      DATA_FILE="{data_file}"
      if [ ! -f "${DATA_FILE}" ]; then
        echo "[train-models] data file ${DATA_FILE} not found."
        exit 1
      fi
      read -r MODELS < "{RUN_DIR}/prepare-run/models.txt"
      PARAM_FILE="{RUN_DIR}/prepare-run/model_overrides.json"
      get_model_params() {
        ARCH_NAME="$1"
        python - <<'PY' "${ARCH_NAME}" "${PARAM_FILE}"
      import json
      import sys
      from pathlib import Path

      arch = sys.argv[1]
      param_file = Path(sys.argv[2])
      defaults = {
          "learning_rate": "{learning_rate}",
          "weight_decay": "{weight_decay}",
          "grad_clip": "{grad_clip}",
          "num_epochs": "{num_epochs}",
          "batch_size": "{batch_size}",
      }
      overrides = {}
      if param_file.is_file():
          try:
              overrides = json.loads(param_file.read_text(encoding="utf-8"))
          except json.JSONDecodeError as exc:
              raise SystemExit(f"[train-models] Invalid overrides file: {exc}")
      config = dict(defaults)
      config.update(overrides.get(arch, {}))
      print(
          "\t".join(
              [
                  str(config["learning_rate"]),
                  str(config["weight_decay"]),
                  str(config["grad_clip"]),
                  str(config["num_epochs"]),
                  str(config["batch_size"]),
              ]
          )
      )
      PY
      }
      for ARCH in ${MODELS}; do
        [ -z "${ARCH}" ] && continue
        PARAM_LINE="$(get_model_params "${ARCH}")"
        if [ -z "${PARAM_LINE}" ]; then
          echo "[train-models] Failed to resolve parameters for ${ARCH}"
          exit 1
        fi
        IFS=$'\t' read -r LR WD CLIP EPOCHS BATCH <<< "${PARAM_LINE}"
        RUN_ROOT="{STEP_DIR}/${ARCH}"
        mkdir -p "${RUN_ROOT}/artifacts" "${RUN_ROOT}/logs" "${RUN_ROOT}/results"
        COMBO="lr${LR}_wd${WD}_clip${CLIP}"
        echo "[train-models] Training ${ARCH} :: ${COMBO}"
        WEIGHT_DECAY=${WD} GRAD_CLIP_MAX_NORM=${CLIP} PYTHONPATH={EXPERIMENT_HOME}/src python {EXPERIMENT_HOME}/src/vox_cpet/cmd/cpetx-model train \
          --data-file "${DATA_FILE}" \
          --conf {CONFIGS_DIR}/model \
          --filter "^${ARCH}$" \
          --run-dir "${RUN_ROOT}" \
          --save-dir "${RUN_ROOT}/artifacts" \
          --log-dir "${RUN_ROOT}/logs" \
          --task-id "{TASK_ID}_${ARCH}_full" \
          --num-epochs "${EPOCHS}" \
          --learning-rate "${LR}" \
          --batch-size "${BATCH}"
        PYTHONPATH={EXPERIMENT_HOME}/src python {EXPERIMENT_HOME}/src/scripts/pipeline_tasks.py write-meta \
          --run-root "${RUN_ROOT}" \
          --arch "${ARCH}" \
          --combo "${COMBO}" \
          --learning-rate "${LR}" \
          --weight-decay "${WD}" \
          --grad-clip "${CLIP}" \
          --stage "full"
      done

  - id: collect-reports
    type: shell
    depends_on: [train-models]
    description: "Emit best-model style reports for downstream ranking."
    vars:
      STEP_DIR: "{RUN_DIR}/collect-reports"
    command: |
      set -e
      python - <<'PY'
      import json
      from datetime import datetime, timezone
      from pathlib import Path

      run_dir = Path("{RUN_DIR}")
      reports_dir = run_dir / "reports"
      reports_dir.mkdir(parents=True, exist_ok=True)

      summary_entries = []
      models_line = (run_dir / "prepare-run" / "models.txt").read_text(encoding="utf-8")
      models = [item for item in models_line.split() if item]
      timestamp = datetime.now(timezone.utc).isoformat()

      for arch in models:
          run_root = run_dir / "train" / arch
          meta_path = run_root / "meta.json"
          combo = f"{arch}_default"
          learning_rate = None
          weight_decay = None
          grad_clip = None
          if meta_path.is_file():
              try:
                  meta = json.loads(meta_path.read_text(encoding="utf-8"))
                  combo = meta.get("combo", combo)
                  learning_rate = meta.get("learning_rate")
                  weight_decay = meta.get("weight_decay")
                  grad_clip = meta.get("grad_clip")
              except json.JSONDecodeError:
                  pass

          candidate_paths = [
              run_root / "artifacts" / arch / "results" / "test_final_results.json",
              run_root / "artifacts" / "results" / "test_final_results.json",
              run_root / "results" / "test_final_results.json",
          ]
          if run_root.is_dir():
              for extra in run_root.glob("**/test_final_results.json"):
                  candidate_paths.append(extra)

          metrics = None
          eval_path = ""
          for candidate in candidate_paths:
              if not candidate.is_file():
                  continue
              try:
                  payload = json.loads(candidate.read_text(encoding="utf-8"))
              except json.JSONDecodeError:
                  continue
              data = payload.get("test_metrics", payload)
              if "mae" in data and "r2_score" in data:
                  metrics = data
                  eval_path = str(candidate.parent)
                  break
          if not metrics:
              continue

          def as_float(value):
              try:
                  return float(value)
              except (TypeError, ValueError):
                  return None

          best_payload = {
              "arch": arch,
              "combo": combo,
              "learning_rate": as_float(learning_rate),
              "weight_decay": as_float(weight_decay),
              "grad_clip": as_float(grad_clip),
              "mae": as_float(metrics.get("mae")),
              "r2_score": as_float(metrics.get("r2_score")),
              "rmse": as_float(metrics.get("rmse")),
              "correlation": as_float(metrics.get("correlation")),
              "mape": as_float(metrics.get("mape")),
              "eval_path": eval_path,
              "generated_at": timestamp,
          }
          best_path = reports_dir / f"{arch}_best_model.json"
          best_path.write_text(json.dumps(best_payload, indent=2, ensure_ascii=False), encoding="utf-8")

          top_payload = {
              "stage": "full",
              "selected": [
                  {
                      "arch": arch,
                      "combo": combo,
                      "learning_rate": as_float(learning_rate),
                      "weight_decay": as_float(weight_decay),
                      "grad_clip": as_float(grad_clip),
                  }
              ],
          }
          top_path = reports_dir / f"{arch}_top_full_candidates.json"
          top_path.write_text(json.dumps(top_payload, indent=2, ensure_ascii=False), encoding="utf-8")

          summary_entries.append(
              {
                  "model": arch,
                  "pipeline_dir": str(run_root),
                  "top_full_candidates.json": str(top_path),
                  "best_model.json": str(best_path),
              }
          )

      summary = {"models": summary_entries}
      (reports_dir / "multi_model_summary.json").write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8")
      PY

  - id: rank-models
    type: shell
    depends_on: [collect-reports]
    description: "Update global ranking based on latest best-model reports."
    vars:
      STEP_DIR: "{RUN_DIR}/rank-models"
    command: |
      set -e
      mkdir -p "{STEP_DIR}"
      python - <<'PY'
      import csv
      import json
      from datetime import datetime
      from pathlib import Path

      def score_tuple(entry):
          def to_float(value, default):
              try:
                  return float(value)
              except (TypeError, ValueError):
                  return default

          mae = to_float(entry.get("mae"), float("inf"))
          rmse = to_float(entry.get("rmse"), float("inf"))
          r2 = entry.get("r2_score")
          r2_component = -to_float(r2, float("inf")) if r2 is not None else float("inf")
          return (mae, rmse, r2_component)

      def should_scan_all(value: str) -> bool:
          return value.strip().lower() in {"1", "true", "yes", "y", "on"}

      def derive_run_name(path: Path) -> str:
          parts = list(path.parts)
          if "sota" in parts:
              idx = parts.index("sota")
              if idx + 1 < len(parts):
                  return parts[idx + 1]
          return "{RUN_TS}"

      run_dir = Path("{RUN_DIR}")
      reports_root = run_dir / "reports"
      step_dir = Path("{STEP_DIR}")
      best_dir = Path("{BEST_DIR}")
      timestamp = datetime.utcnow().isoformat() + "Z"
      scan_all = should_scan_all("{rank_scan_all}")

      reports_root.mkdir(parents=True, exist_ok=True)
      best_dir.mkdir(parents=True, exist_ok=True)

      sources = []
      if scan_all:
          runs_root = Path("{RUNS_DIR}")
          if runs_root.exists():
              sources.extend(sorted(runs_root.glob("*/reports/*_best_model.json")))
      else:
          sources.extend(sorted(reports_root.glob("*_best_model.json")))

      records = []
      for best_file in sources:
          try:
              data = json.loads(best_file.read_text(encoding="utf-8"))
          except json.JSONDecodeError:
              continue

          entry = {
              "model": data.get("arch") or best_file.stem.replace("_best_model", ""),
              "combo": data.get("combo"),
              "learning_rate": data.get("learning_rate"),
              "weight_decay": data.get("weight_decay"),
              "grad_clip": data.get("grad_clip"),
              "mae": data.get("mae"),
              "rmse": data.get("rmse"),
              "r2_score": data.get("r2_score"),
              "correlation": data.get("correlation"),
              "mape": data.get("mape"),
              "subset_data": "{subset_data}",
              "full_data": "{full_data}",
              "sprint_epochs": "{sprint_epochs}",
              "full_epochs": "{full_epochs}",
              "source_file": str(best_file),
              "source_run": derive_run_name(best_file),
              "generated_at": data.get("generated_at"),
              "updated_at": timestamp,
          }
          records.append(entry)

      ranked_entries = []
      for rank, entry in enumerate(sorted(records, key=score_tuple), start=1):
          ranked = dict(entry)
          ranked["rank"] = rank
          ranked_entries.append(ranked)

      run_payload = {
          "generated_at": timestamp,
          "source_run": "{RUN_TS}",
          "scan_all": scan_all,
          "entries": ranked_entries,
      }

      run_json = step_dir / "model_ranking.json"
      run_json.write_text(json.dumps(run_payload, indent=2, ensure_ascii=False), encoding="utf-8")

      run_csv = step_dir / "model_ranking.csv"
      with run_csv.open("w", newline="", encoding="utf-8") as csvfile:
          writer = csv.writer(csvfile)
          writer.writerow(["rank", "model", "mae", "rmse", "r2_score", "mape", "correlation", "combo", "learning_rate", "weight_decay", "grad_clip", "subset_data", "full_data", "sprint_epochs", "full_epochs", "source_file", "source_run"])
          for entry in ranked_entries:
              writer.writerow([
                  entry.get("rank"),
                  entry.get("model"),
                  entry.get("mae"),
                  entry.get("rmse"),
                  entry.get("r2_score"),
                  entry.get("mape"),
                  entry.get("correlation"),
                  entry.get("combo"),
                  entry.get("learning_rate"),
                  entry.get("weight_decay"),
                  entry.get("grad_clip"),
                  entry.get("subset_data"),
                  entry.get("full_data"),
                  entry.get("sprint_epochs"),
                  entry.get("full_epochs"),
                  entry.get("source_file"),
                  entry.get("source_run"),
              ])

      last_run_path = best_dir / "model_ranking_last_run.json"
      last_run_path.write_text(json.dumps(run_payload, indent=2, ensure_ascii=False), encoding="utf-8")

      best_path = best_dir / "model_ranking.json"
      best_data = {}
      if best_path.exists():
          try:
              existing = json.loads(best_path.read_text(encoding="utf-8"))
              for item in existing.get("entries", []):
                  model = item.get("model")
                  if model:
                      best_data[model] = item
          except json.JSONDecodeError:
              pass

      for entry in ranked_entries:
          model = entry.get("model")
          if not model:
              continue
          candidate = dict(entry)
          candidate["updated_at"] = timestamp
          current = best_data.get(model)
          if current is None or score_tuple(candidate) < score_tuple(current):
              best_data[model] = candidate

      best_entries = sorted(best_data.values(), key=score_tuple)
      for idx, item in enumerate(best_entries, start=1):
          item["rank"] = idx

      best_payload = {
          "generated_at": timestamp,
          "entries": best_entries,
      }

      best_path.write_text(json.dumps(best_payload, indent=2, ensure_ascii=False), encoding="utf-8")

      best_csv = best_dir / "model_ranking.csv"
      with best_csv.open("w", newline="", encoding="utf-8") as csvfile:
          writer = csv.writer(csvfile)
          writer.writerow(["rank", "model", "mae", "rmse", "r2_score", "mape", "correlation", "combo", "learning_rate", "weight_decay", "grad_clip", "subset_data", "full_data", "sprint_epochs", "full_epochs", "source_run", "source_file", "updated_at"])
          for entry in best_entries:
              writer.writerow([
                  entry.get("rank"),
                  entry.get("model"),
                  entry.get("mae"),
                  entry.get("rmse"),
                  entry.get("r2_score"),
                  entry.get("mape"),
                  entry.get("correlation"),
                  entry.get("combo"),
                  entry.get("learning_rate"),
                  entry.get("weight_decay"),
                  entry.get("grad_clip"),
                  entry.get("subset_data"),
                  entry.get("full_data"),
                  entry.get("sprint_epochs"),
                  entry.get("full_epochs"),
                  entry.get("source_run"),
                  entry.get("source_file"),
                  entry.get("updated_at"),
              ])

      for entry in best_entries:
          model = entry.get("model")
          if not model:
              continue
          (best_dir / f"{model}.json").write_text(json.dumps(entry, indent=2, ensure_ascii=False), encoding="utf-8")
      PY
